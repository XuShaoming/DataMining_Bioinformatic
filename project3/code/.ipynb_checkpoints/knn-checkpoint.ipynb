{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from io import StringIO\n",
    "import mylibrary as mylib\n",
    "import heapq\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Record:\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Use to save the label_id of the nearest neighbor of given data entry.\n",
    "        self.label_id save a neighbor's label\n",
    "        self.distance save the distance between the neighbor and the data entry\n",
    "        self.row_id save the neighbor's row_id in training data. For debug purpose\n",
    "    \"\"\"\n",
    "    def __init__(self, label_id, distance, row_id):\n",
    "        self.label_id = label_id\n",
    "        self.distance = distance\n",
    "        self.row_id = row_id\n",
    "    #inverse for max heap\n",
    "    def __eq__(self, other):\n",
    "        return self.distance - other.distance == 0\n",
    "    def __ne__(self, other):\n",
    "        return self.distance - other.distance != 0\n",
    "    def __lt__(self, other):\n",
    "        return self.distance - other.distance > 0\n",
    "    def __le__(self, other):\n",
    "        return self.distance - other.distance >= 0\n",
    "    def __gt__(self, other):\n",
    "        return self.distance - other.distance < 0\n",
    "    def __ge__(self, other):\n",
    "        return self.distance - other.distance >= 0\n",
    "    def __repr__(self):\n",
    "         return \"row_id:\" + str(self.row_id) + \"  label_id:\" + str(self.label_id) + \"  distance:\" + str(self.distance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        read file from given path. Then get data and label from file.\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        raw_data = np.genfromtxt(StringIO(f.read()), delimiter=\"\\t\", dtype='str')\n",
    "    label = raw_data[:,-1].astype(int)\n",
    "    data = raw_data[:,:-1]\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(arr, new_min, new_max) :\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        normalize a list of number in range (new_min,new_max).\n",
    "    Input:\n",
    "        arr: real list, or real np.array\n",
    "        new_mim: real\n",
    "        new_max: real\n",
    "    output:\n",
    "        the normalized numpy array\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    old_min = np.min(arr)\n",
    "    old_max = np.max(arr)\n",
    "    old_range = old_max - old_min\n",
    "    new_range = new_max - new_min\n",
    "    return (((arr - old_min) * new_range) / old_range) + new_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(data):\n",
    "    \"\"\"\n",
    "    Purpose: scale the data in range 0 to 1. For non-numerical data, we represent its element as\n",
    "    id, then scare it to range 0 to 1. \n",
    "    \n",
    "    Input:\n",
    "        data: 2-D numpy array\n",
    "    Output:\n",
    "        the scaled data\n",
    "        nan_cols: a list of column id that recrods the non_numbercal data.\n",
    "    \"\"\"\n",
    "    data_t = []\n",
    "    nan_cols = []\n",
    "    for i, row in enumerate(data.T):\n",
    "        try:\n",
    "            float(row[0])\n",
    "            row = row.astype(float)\n",
    "            data_t.append(scale(row,0,1))\n",
    "        except ValueError:\n",
    "            nan_cols.append(i)\n",
    "            elems = list(set(row))\n",
    "            new_row = np.asarray([elems.index(x) for x in row])\n",
    "            data_t.append(scale(new_row,0,1))\n",
    "            \n",
    "    return np.asarray(data_t).T, np.asarray(nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_fold(data, label, n, pos):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        divide data to n blocks.\n",
    "        extact one block in start from pos for test set\n",
    "        the other n-1 block as the training set.\n",
    "    Input:\n",
    "        data: 2-D numpy array\n",
    "        label: 1-D numpy array\n",
    "        n: number of fold\n",
    "        pos: the id that shows the block as the test set.\n",
    "    Output:\n",
    "        tranining_data: 2-D numpy array\n",
    "        training_label: 1-D numpy array\n",
    "        test_data: 2-D numpy array\n",
    "        test_label: 1-D numpy array\n",
    "    \"\"\"\n",
    "    block_size = int(data.shape[0] / n)\n",
    "    training_data = np.vstack((data[0:block_size*pos], data[block_size*(pos+1):]))\n",
    "    test_data = data[block_size*pos: block_size*(pos+1)]\n",
    "    training_label =  np.append(label[0:block_size*pos], label[block_size*(pos+1):])\n",
    "    test_label = label[block_size*pos: block_size*(pos+1)]\n",
    "    \n",
    "    return training_data, training_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pt1, pt2, cols_nan = None):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        get the euclidean distance between two records. The cols_nan store the id of\n",
    "        non-numerical data column. for these column, if two records are the same, \n",
    "        distance is 0. Otherwise is 1.\n",
    "    Input:\n",
    "        pt1: 1-D numpy array. a data record\n",
    "        pt2: 1-D numpy array. a data record\n",
    "        cols_nan: 1-D numpy array. IDs of non-numerical data.\n",
    "    Output:\n",
    "        real: distance between two record.\n",
    "        \n",
    "    \"\"\"\n",
    "    if cols_nan != None and len(cols_nan) > 0:\n",
    "        cols_nan = np.asarray(cols_nan)\n",
    "        total = 0\n",
    "        for i, val in enumerate(pt1):\n",
    "            if np.any(cols_nan == i):\n",
    "                if pt1[i] != pt2[i]:\n",
    "                    total += 1\n",
    "            else:\n",
    "                total += np.square(pt1[i] - pt2[i])\n",
    "        return np.sqrt(total)\n",
    "    \n",
    "    return np.sqrt(np.sum(np.square(pt1 - pt2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "list(map(lambda x: x*2, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(k, training_data, training_label, label):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        a high order function that return a funcion to do knn predict.\n",
    "    \"\"\"\n",
    "    def predict(test_data):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            the function to do prediction on test_data.\n",
    "        \"\"\"\n",
    "        res_label = []\n",
    "        label_set = set(label)\n",
    "        for row in test_data:\n",
    "            heap = []\n",
    "            for i,check in enumerate(training_data):\n",
    "                heapq.heappush(heap,Record(training_label[i], euclidean_distance(row, check, cols_nan=nan_cols), i))\n",
    "                if(len(heap) > k):\n",
    "                    heapq.heappop(heap)\n",
    "            vote_list = [obj.label_id for obj in heap]\n",
    "            res_label.append(max(vote_list, key=Counter(vote_list).get))\n",
    "        return np.asarray(res_label)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(test_label, res_label):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        get the confusion_matrix\n",
    "        res[0][0] is the True Postive(TP)\n",
    "        res[0][1] is the False Negative(FN)\n",
    "        res[1][0] is the False Positive(FP)\n",
    "        res[1][1] is the True Negative(TN)\n",
    "    \"\"\"\n",
    "    res = [[0,0],[0,0]]\n",
    "    for i, val in enumerate(test_label):\n",
    "        if val == 1:\n",
    "            if val == res_label[i]:\n",
    "                res[0][0] += 1\n",
    "            else:\n",
    "                res[0][1] += 1\n",
    "        else:\n",
    "            if val != res_label[i]:\n",
    "                res[1][0] += 1\n",
    "            else:\n",
    "                res[1][1] += 1\n",
    "    return np.asarray(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(confusion):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        accuracy = (TP + TN)/(TP+TN+FP+FN)\n",
    "    \"\"\"\n",
    "    return (confusion[0,0] + confusion[1,1]) / np.sum(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(confusion):\n",
    "    \"\"\"\n",
    "    PRECISION = TP / (TP + FP)\n",
    "    \"\"\"\n",
    "    return confusion[0,0] / (confusion[0,0] + confusion[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(confusion):\n",
    "    \"\"\"\n",
    "    Recall = TP / (TP + FN)\n",
    "    \"\"\"\n",
    "    return confusion[0,0] / (confusion[0,0] + confusion[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(confusion):\n",
    "    \"\"\"\n",
    "    F1 score = 2*TP / (2*TP + FN + FP)\n",
    "    \"\"\"\n",
    "    return 2 * confusion[0,0] / (2 * confusion[0,0] + confusion[0,1] + confusion[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = get_data(\"../data/project3_dataset2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess\n",
    "mat = data\n",
    "data, nan_cols = scale_features(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** itr:0 *****\n",
      "Accuracy:  0.5869565217391305\n",
      "Precision:  0.5555555555555556\n",
      "Recall:  0.47619047619047616\n",
      "F1-Score:  0.5128205128205128\n",
      "***** itr:1 *****\n",
      "Accuracy:  0.6304347826086957\n",
      "Precision:  0.38095238095238093\n",
      "Recall:  0.6666666666666666\n",
      "F1-Score:  0.48484848484848486\n",
      "***** itr:2 *****\n",
      "Accuracy:  0.6956521739130435\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.5263157894736842\n",
      "F1-Score:  0.5882352941176471\n",
      "***** itr:3 *****\n",
      "Accuracy:  0.7608695652173914\n",
      "Precision:  0.7\n",
      "Recall:  0.4666666666666667\n",
      "F1-Score:  0.56\n",
      "***** itr:4 *****\n",
      "Accuracy:  0.5869565217391305\n",
      "Precision:  0.625\n",
      "Recall:  0.23809523809523808\n",
      "F1-Score:  0.3448275862068966\n",
      "***** itr:5 *****\n",
      "Accuracy:  0.6304347826086957\n",
      "Precision:  0.5555555555555556\n",
      "Recall:  0.2777777777777778\n",
      "F1-Score:  0.37037037037037035\n",
      "***** itr:6 *****\n",
      "Accuracy:  0.6739130434782609\n",
      "Precision:  0.4\n",
      "Recall:  0.3076923076923077\n",
      "F1-Score:  0.34782608695652173\n",
      "***** itr:7 *****\n",
      "Accuracy:  0.8043478260869565\n",
      "Precision:  0.5714285714285714\n",
      "Recall:  0.4\n",
      "F1-Score:  0.47058823529411764\n",
      "***** itr:8 *****\n",
      "Accuracy:  0.5869565217391305\n",
      "Precision:  0.38461538461538464\n",
      "Recall:  0.3125\n",
      "F1-Score:  0.3448275862068966\n",
      "***** itr:9 *****\n",
      "Accuracy:  0.6521739130434783\n",
      "Precision:  0.4166666666666667\n",
      "Recall:  0.35714285714285715\n",
      "F1-Score:  0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "k = 5\n",
    "for i in np.arange(10):\n",
    "    print(\"***** itr:\" + str(i)+\" *****\")\n",
    "    training_data, training_label, test_data, test_label = n_fold(data, label, n, i)\n",
    "    predict = model(k, training_data, training_label, label)\n",
    "    res_label = predict(test_data)\n",
    "    conf_mat = confusion_matrix(test_label, res_label)\n",
    "    print(\"Accuracy: \", get_accuracy(conf_mat))\n",
    "    print(\"Precision: \", get_precision(conf_mat))\n",
    "    print(\"Recall: \", get_recall(conf_mat))\n",
    "    print(\"F1-Score: \", get_f1_score(conf_mat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
